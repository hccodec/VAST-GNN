{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model\n",
    "import torch\n",
    "from utils.test import test\n",
    "from eval import compute_err\n",
    "\n",
    "def test_model(model_dir = \"results_2025_1/test_0106_all/exp_2_50_all/dataforgood/dynst_7_1_w7_s2_20250106163802/model_EN_best.pth\"):\n",
    "    # model_dir = \"results_2025_1/tests_1209/exp_2_sim_graph_lambda_0/sim/dynst_7_1_w7_s2_20241209105111/model_S3_best.pth\"\n",
    "    res, meta_data, args = test(model_dir, logger_disable=True, device=4)\n",
    "    ((loss_train, y_real_train, y_hat_train, adj_real_train, adj_hat_train),\n",
    "                        (loss_val, y_real_val, y_hat_val, adj_real_val, adj_hat_val),\n",
    "                        (loss_test, y_real_test, y_hat_test, adj_real_test, adj_hat_test)) = [map(lambda x: x.detach().cpu().numpy() if isinstance(x, torch.Tensor) else x, r) for r in res['outputs']]\n",
    "    # err_val, err_test = compute_err(y_hat_val, y_real_val, False), compute_err(y_hat_test, y_real_test, False)\n",
    "    errs_test = [compute_err(y_hat_test[:, :, i, :], y_real_test[:, :, i, :], False) for i in range(y_hat_test.shape[2])]\n",
    "    cur = locals()\n",
    "    # return errs_test\n",
    "    return {v: cur[v] for v in (\"loss_train\", \"y_real_train\", \"y_hat_train\", \"adj_real_train\", \"adj_hat_train\",\n",
    "                        \"loss_val\", \"y_real_val\", \"y_hat_val\", \"adj_real_val\", \"adj_hat_val\",\n",
    "                        \"loss_test\", \"y_real_test\", \"y_hat_test\", \"adj_real_test\", \"adj_hat_test\", \"errs_test\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5304e648c47243efa9a307ea46a42274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "552bf51407774bcea2852bad15193698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 画 散点图\n",
    "\n",
    "# from utils.utils import matplotlib_chinese\n",
    "# matplotlib_chinese()\n",
    "import os, numpy as np\n",
    "from utils.utils import get_exp_desc\n",
    "from tqdm.auto import tqdm\n",
    "# 用 matplotlib 画出散点图。并标出对角线（y=x）\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "fontsize = 14\n",
    "\n",
    "def plot(ydays, dataset, country_code, node_observation_ratio, filepath, has_inset=False):\n",
    "\n",
    "    errs = {model_name: test_model(b.path)['errs_test'] for model_name, b in paths[f'o{node_observation_ratio}'].sort_index().loc[(ydays, country_code)].iterrows()}\n",
    "    maes = {model_name: b.mae for model_name, b in paths[f'o{node_observation_ratio}'].sort_index().loc[(ydays, country_code)].iterrows()}\n",
    "\n",
    "    model_names = list(errs.keys())\n",
    "\n",
    "    xs, y = [errs[model_names[i]] for i in range(1, len(model_names))], errs[model_names[0]]\n",
    "\n",
    "    fig, axes = plt.subplots(len(xs), 1, figsize=(8, 15))\n",
    "    # fig, axes = plt.subplots(1, len(xs), figsize=(18, 5))\n",
    "\n",
    "    # fig.suptitle(get_exp_desc(f'({country_code}) ', 7, 1, 7, ydays - 1, node_observation_ratio, 'en'), fontsize=fontsize)\n",
    "\n",
    "    [a.plot([0, max(x + y)], [0, max(x + y)],\n",
    "            color='grey', linestyle='-.', linewidth=1.5, alpha=0.7) for x, a in zip(xs, axes)] # x = y\n",
    "\n",
    "    # [a.scatter(xs[i], y, color='darkgreen', s=50, alpha=0.6) for i, a in enumerate(axes)]\n",
    "    # 区分对角线两侧的散点颜色\n",
    "    for i, a in enumerate(axes):\n",
    "        x = np.array(xs[i])  # 转为 numpy 数组便于比较\n",
    "        y_array = np.array(y)\n",
    "        \n",
    "        mask_above = y_array > x\n",
    "        mask_below = y_array < x\n",
    "        mask_equal = ~(mask_below | mask_above)  # 如果有相等的情况\n",
    "        \n",
    "        a.scatter(x[mask_above], y_array[mask_above], color='green', s=50, alpha=0.4, label='y > x') # 上三角 (y > x): 浅绿色\n",
    "        a.scatter(x[mask_below], y_array[mask_below], color='green', s=50, alpha=0.8, label='y < x') # 下三角 (y < x): 深绿色\n",
    "        if mask_equal.any(): a.scatter(x[mask_equal], y_array[mask_equal], color='grey', s=50, alpha=0.8, label='y = x') # 对角线附近 (y ≈ x): 中间色（可选）\n",
    "\n",
    "        if has_inset:\n",
    "            # 创建嵌入子图，显示极值（100-150范围）\n",
    "            inset = inset_axes(a, width=\"30%\", height=\"30%\", loc='upper right')  # 右上角\n",
    "            inset.scatter(x[mask_above], y_array[mask_above], color='green', s=50, alpha=0.4)\n",
    "            inset.scatter(x[mask_below], y_array[mask_below], color='green', s=50, alpha=0.8)\n",
    "            if mask_equal.any(): inset.scatter(x[mask_equal], y_array[mask_equal], color='grey', s=50, alpha=0.8)\n",
    "\n",
    "            xlim, ylim = a.get_xlim(), a.get_ylim()\n",
    "            threshold = np.percentile(np.concatenate([x, y_array]), 90)  # 取90分位数作为阈值\n",
    "            \n",
    "            a.set_xlim(xlim[0], threshold * 1.7)\n",
    "            a.set_ylim(ylim[0], threshold * 1.7)\n",
    "            inset.spines['top'].set_color('black')\n",
    "            inset.spines['bottom'].set_color('black')\n",
    "            inset.spines['left'].set_color('black')\n",
    "            inset.spines['right'].set_color('black')\n",
    "            inset.set_xlim(threshold, max(x.max(), y_array.max()) * 1.1)\n",
    "            inset.set_ylim(threshold, max(x.max(), y_array.max()) * 1.1)\n",
    "        \n",
    "        a.tick_params(axis='both', labelsize=fontsize, pad=5)\n",
    "\n",
    "        # 隐藏顶部和右侧边框\n",
    "        a.spines['top'].set_visible(False)\n",
    "        a.spines['right'].set_visible(False)\n",
    "        a.spines['bottom'].set_visible(False)\n",
    "        a.spines['left'].set_visible(False)\n",
    "\n",
    "        # 绘制箭头\n",
    "        a.annotate('', xy=(1.1, 0), xycoords='axes fraction', xytext=(-0.1, 0), textcoords='axes fraction',\n",
    "                   arrowprops=dict(facecolor='black', shrink=0.05, width=0.5, linewidth=0.5), zorder=5)\n",
    "        a.annotate('', xy=(0, 1.1), xycoords='axes fraction', xytext=(0, -0.1), textcoords='axes fraction',\n",
    "                   arrowprops=dict(facecolor='black', shrink=0.05, width=0.5, linewidth=0.5), zorder=5)\n",
    "\n",
    "    [a.set_xlabel(\"MAE of %s\" % model_names[i + 1].upper(), fontsize=fontsize) for i, a in enumerate(axes)]\n",
    "    [a.set_ylabel(\"MAE of VAST-GNN\", fontsize=fontsize) for a in axes]\n",
    "    # [a.set_ylabel(\"MAE of %s\" % model_names[0].capitalize(), fontsize=fontsize) for a in axes]\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.5, hspace=None) # 调整水平间距\n",
    "    fig.savefig(filepath, transparent=True)\n",
    "    plt.close()\n",
    "    # plt.show()\n",
    "\n",
    "DIR_NAME = \"visualization/results_plot_maes\"\n",
    "\n",
    "def draw_main(dataset_name, ratios, ydays, countries):\n",
    "    qbar = tqdm(total=len(ratios) * len(ydays) * len(countries))\n",
    "    for ratio in ratios:\n",
    "        os.makedirs(DIR_NAME, exist_ok=True)\n",
    "        for y in ydays:\n",
    "            for country in countries:\n",
    "                filename = \"_\".join(map(str, (y, dataset_name, country, ratio))) + \".pdf\"\n",
    "                plot(y, dataset_name, country, ratio, os.path.join(DIR_NAME, filename))\n",
    "                qbar.update()\n",
    "\n",
    "from best_results import paths\n",
    "draw_main(dataset_name=\"dataforgood\", ratios=[50, 80], ydays=[3, 7, 14], countries=[\"EN\", \"FR\", \"IT\", \"ES\", \"NZ\", \"JP\"])\n",
    "from best_results import paths_flunet as paths\n",
    "draw_main(dataset_name=\"flunet\", ratios=[50], ydays=[3], countries=[\"h1n1\", \"h3n2\", \"BY\", \"BV\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from utils.test import test\n",
    "# from eval import compute_err\n",
    "\n",
    "\n",
    "# def test_model(model_path):\n",
    "#     res, meta_data, args = test(model_path)\n",
    "#     (\n",
    "#         (loss_train, y_real_train, y_hat_train, adj_real_train, adj_hat_train),\n",
    "#         (loss_val, y_real_val, y_hat_val, adj_real_val, adj_hat_val),\n",
    "#         (loss_test, y_real_test, y_hat_test, adj_real_test, adj_hat_test),\n",
    "#     ) = [\n",
    "#         map(lambda x: x.detach().cpu().numpy() if isinstance(x, torch.Tensor) else x, r)\n",
    "#         for r in res[\"outputs\"]\n",
    "#     ]\n",
    "#     err_test =  compute_err(y_hat_test, y_real_test, False)\n",
    "#     return f\"{err_test:.3f}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 重新导入包 importlib\n",
    "# if 'best_results' in globals():\n",
    "#     print('重新导入包')\n",
    "#     import importlib\n",
    "#     importlib.reload(best_results)\n",
    "# else:\n",
    "#     import best_results\n",
    "\n",
    "# paths = best_results.paths\n",
    "\n",
    "# import zipfile\n",
    "# from tqdm.auto import trange\n",
    "\n",
    "# zip_file_name = 'best_results.zip'\n",
    "\n",
    "# with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n",
    "#     qbar = trange(len(paths['o50']))\n",
    "#     for i in qbar:\n",
    "#         data = paths['o50'].iloc[i]\n",
    "#         qbar.set_description(\" \".join(map(str, data.name)))\n",
    "#         zipf.write(data.path, f\"y{'_'.join(map(str, data.name))}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from best_results import paths\n",
    "# paths[f'o{node_observation_ratio}'].sort_index().loc[(ydays, country_code)].iterrows()\n",
    "import re\n",
    "paths['o50']['path'].iloc[i * 4 - 1]\n",
    "dataset, model, ydays_minus_1, country = re.search(r\".*/(.*?)/(.*?)_\\d+_\\d+_w\\d+_s(\\d+)_.*?/model_(.*)_best.pth\", paths['o50']['path'].iloc[i * 4 - 1]).groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from best_results import paths\n",
    "\n",
    "paths['o50']['path'].iloc[3]\n",
    "\n",
    "len(paths['o50']['path']) / 4\n",
    "\n",
    "# res = test_model(path)\n",
    "# res['adj_hat_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画 矩阵热图\n",
    "\n",
    "import numpy as np, re\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "from best_results import paths\n",
    "# from best_results import paths_flunet as paths\n",
    "\n",
    "_index = 0\n",
    "path = paths['o50']['path'].iloc[_index * 4 - 1]\n",
    "dataset, model, ydays_minus_1, country = re.search(r\".*/(.*?)/(.*?)_\\d+_\\d+_w\\d+_s(\\d+)_.*?/model_(.*)_best.pth\", path).groups()\n",
    "res = test_model(path)\n",
    "adj_real_test, adj_hat_test = res['adj_real_test'], res['adj_hat_test']\n",
    "\n",
    "# 创建掩码函数\n",
    "def create_alpha_mask(matrix):\n",
    "    n = matrix.shape[0]\n",
    "    alpha = np.ones((n, n))\n",
    "    np.fill_diagonal(alpha, 0)  # 设置对角线为透明\n",
    "    return alpha\n",
    "\n",
    "# 创建交互式控件\n",
    "# widgets.SelectionSlider()\n",
    "index_slider = widgets.IntSlider(value=2, min=0, max=len(paths['o50']['path']) / 4 - 1, step=1, description='Index')\n",
    "batch_slider = widgets.IntSlider(value=6, min=0, max=adj_real_test.shape[0] - 1, step=1, description='Batch')\n",
    "day_slider = widgets.IntSlider(value=1, min=0, max=adj_real_test.shape[1] - 1, step=1, description='Day')\n",
    "\n",
    "# 绘图函数\n",
    "def update(index, batch, day):\n",
    "    global _index, path, dataset, model, ydays_minus_1, country, res, adj_real_test, adj_hat_test\n",
    "    if _index != index:\n",
    "        path = paths['o50']['path'].iloc[index * 4 - 1]\n",
    "        dataset, model, ydays_minus_1, country = re.search(r\".*/(.*?)/(.*?)_\\d+_\\d+_w\\d+_s(\\d+)_.*?/model_(.*)_best.pth\", path).groups()\n",
    "        res = test_model(path)\n",
    "        adj_real_test, adj_hat_test = res['adj_real_test'], res['adj_hat_test']\n",
    "        batch_slider.max = adj_real_test.shape[0] - 1\n",
    "        day_slider.max = adj_real_test.shape[1] - 1\n",
    "        batch_slider.value = 0\n",
    "        day_slider.value = 0\n",
    "        _index = index\n",
    "    \n",
    "    real_matrix = adj_real_test[batch, day]\n",
    "    hat_matrix = adj_hat_test[batch, day]\n",
    "    \n",
    "    # 创建透明掩码\n",
    "    alpha_mask_real = create_alpha_mask(real_matrix)\n",
    "    alpha_mask_hat = create_alpha_mask(hat_matrix)\n",
    "    \n",
    "    # 绘制图像\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    fig.suptitle(f\"[{'_'.join((dataset, model, ydays_minus_1, country))}] Index {index}, Batch {batch}, Day {day}\", fontsize=16)\n",
    "\n",
    "    hat_matrix = (hat_matrix - hat_matrix.min()) / (hat_matrix.max() - hat_matrix.min())\n",
    "    # 绘制预测值图像\n",
    "    im0 = axes[0].imshow(hat_matrix, cmap='magma', interpolation='nearest', vmin=hat_matrix[hat_matrix > 0].min(), vmax=hat_matrix[hat_matrix > 0].max())\n",
    "    fig.colorbar(im0, ax=axes[0], label='Value')\n",
    "    axes[0].imshow(hat_matrix, cmap='magma', interpolation='nearest', alpha=alpha_mask_hat)  # 应用透明掩码\n",
    "    axes[0].axis('off')\n",
    "    axes[0].set_title('Ours')\n",
    "\n",
    "    # 绘制真实值图像\n",
    "    im1 = axes[1].imshow(real_matrix, cmap='magma', interpolation='nearest')\n",
    "    fig.colorbar(im1, ax=axes[1], label='Value')\n",
    "    axes[1].imshow(real_matrix, cmap='magma', interpolation='nearest', alpha=alpha_mask_real)  # 应用透明掩码\n",
    "    axes[1].axis('off')\n",
    "    axes[1].set_title('GT')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "interactive_plot = widgets.interactive(update, index=index_slider, batch=batch_slider, day=day_slider)\n",
    "display(interactive_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 参数设置\n",
    "num_nodes = 70  # 节点数\n",
    "num_dates = 120  # 时间步数\n",
    "delta_t = 1.0  # 时间步长\n",
    "a, b, c = 0.074, 0.130, 0.01  # 动力学参数\n",
    "gamma = 0.05  # 网络调整系数\n",
    "max_cases = 1000  # 假设一个病例数上限\n",
    "\n",
    "# 初始化节点和时间\n",
    "nodes = [f\"R{i:03}\" for i in range(num_nodes)]\n",
    "dates = [f\"D{i:03}\" for i in range(num_dates)]\n",
    "\n",
    "# 初始化病例数增长量和网络结构\n",
    "growth = np.zeros((num_dates, num_nodes, 1))  # 每日增长量\n",
    "cases = np.zeros((num_dates, num_nodes, 1))  # 累积病例数\n",
    "adjs = np.random.rand(num_dates, num_nodes, num_nodes)  # 随机生成的邻接矩阵\n",
    "np.fill_diagonal(adjs[0], 0)  # 对角线置零，防止自环\n",
    "\n",
    "# 设置初始病例增长量\n",
    "growth[0] = np.random.randint(1, 10, size=(num_nodes, 1))\n",
    "cases[0] = growth[0]  # 初始病例数为增长量\n",
    "\n",
    "# 定义sigmoid函数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# 动力学公式，计算增长量\n",
    "def compute_growth(x, adj, a, b, c, max_cases):\n",
    "    x_diff = x.T - x  # x_j - x_i\n",
    "    propagation = np.sum(adj * sigmoid(x_diff), axis=1, keepdims=True)  # 邻接矩阵加权传播\n",
    "    growth_rate = a * x + b * propagation - c * x**2  # 基础增长动力学\n",
    "    growth_rate = np.maximum(growth_rate, 0)  # 确保增长量非负\n",
    "    \n",
    "    # 引入Sigmoid增长限制\n",
    "    growth_limit_factor = 1 - sigmoid((x - max_cases) / 100)  # 控制增长到上限的限制项\n",
    "    growth_rate *= growth_limit_factor  # 应用增长限制\n",
    "    return growth_rate\n",
    "\n",
    "# 网络更新规则\n",
    "def update_adjacency(adj, x, gamma):\n",
    "    x_diff = x - x.T  # x_i - x_j\n",
    "    adj += gamma * np.exp(-x_diff**2 / 2)  # 基于高斯核调整权重\n",
    "    np.fill_diagonal(adj, 0)  # 确保对角线仍为0\n",
    "    return np.clip(adj, 0, 1)  # 限制权重范围在[0, 1]\n",
    "\n",
    "# 动力学迭代\n",
    "for i_date in range(1, num_dates):\n",
    "    # 当前时间步病例数和网络结构\n",
    "    x_t = cases[i_date - 1]\n",
    "    adj_t = adjs[i_date - 1]\n",
    "    \n",
    "    # 计算每日增长量\n",
    "    growth_rate = compute_growth(x_t, adj_t, a, b, c, max_cases)\n",
    "    growth[i_date] = delta_t * growth_rate\n",
    "    \n",
    "    # 累加增长量得到病例数\n",
    "    cases[i_date] = cases[i_date - 1] + growth[i_date]\n",
    "    \n",
    "    # 更新网络结构\n",
    "    adjs[i_date] = update_adjacency(adj_t, x_t, gamma)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(cases.shape[0]), cases[:, 5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open(\"dataset_cache_bak/dataforgood_x7_y1_w7_s2_m50.bin\", \"rb\") as f: res = pickle.load(f)\n",
    "with open(\"dataset_cache/sim_x7_y1_w7_s2_m50.bin\", \"rb\") as f: res = pickle.load(f)\n",
    "features, cases, adjs = res['data'][\"SIM1\"][1]\n",
    "\n",
    "cases = cases[:, 6].cumsum(0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(cases.shape[0]), cases)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 参数设置\n",
    "num_nodes = 70  # 节点数\n",
    "num_dates = 64  # 时间步数\n",
    "delta_t = 1.0  # 时间步长\n",
    "a, b = 0.064, 0.01  # 动力学参数\n",
    "\n",
    "# 初始化节点和时间\n",
    "nodes = [f\"R{i:03}\" for i in range(num_nodes)]\n",
    "dates = [f\"D{i:03}\" for i in range(num_dates)]\n",
    "\n",
    "# 初始化病例数和邻接矩阵\n",
    "cases = np.zeros((num_dates, num_nodes, 1))  # 病例数，形状为 (时间, 节点, 1)\n",
    "adjs = np.random.rand(num_dates, num_nodes, num_nodes)  # 随机生成的邻接矩阵\n",
    "\n",
    "# 设置初始病例数\n",
    "cases[0] = np.random.randint(0, 10, size=(num_nodes, 1))\n",
    "\n",
    "# 定义动力学公式 dx/dt\n",
    "def compute_dx_dt(x, adj, a, b):\n",
    "    \"\"\"\n",
    "    计算 dx/dt = a * x + b * sum(adj * sigmoid(x_j - x_i))\n",
    "    \"\"\"\n",
    "    # 计算 sigmoid(x_j - x_i)\n",
    "    x_diff = x.T - x  # 广播减法\n",
    "    sigmoid = 1 / (1 + np.exp(-x_diff))    \n",
    "    # 计算加权求和部分\n",
    "    weighted_sum = np.sum(adj * sigmoid, axis=1, keepdims=True)\n",
    "    # 返回 dx/dt\n",
    "    return a * x + b * weighted_sum\n",
    "\n",
    "# 动力学迭代\n",
    "for i_date in range(1, num_dates):\n",
    "    # 取前一天的病例数和邻接矩阵\n",
    "    x_t = cases[i_date - 1]\n",
    "    adj_t = adjs[i_date - 1]\n",
    "    # 计算 dx/dt\n",
    "    dx = compute_dx_dt(x_t, adj_t, a, b)\n",
    "    # 使用欧拉法更新病例数\n",
    "    cases[i_date] = x_t + delta_t * dx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(num_dates), cases[:, 0])\n",
    "plt.show()\n",
    "','.join(map(str, np.array(cases[0], dtype=int).squeeze(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 针对 dynst 实验 剥离出 lambda\n",
    "import os, re\n",
    "res = {}\n",
    "for dirname, dirpath, files in os.walk('/home/hbj/workspace/vscode_workspace/lp/workspace/results/tests_1129/exp_3_dynst_lambdas/dataforgood'):\n",
    "    if len(files) != 2 or not all([f.endswith('.txt') for f in files]): continue\n",
    "    # 从 dirname 获取 shift, country，从 args.txt 获取 lambda，从 log.txt 获取 epoch 和 MAE\n",
    "    match = re.search(r\"s(\\d+)_.*?/(.*)\", dirname)\n",
    "    shift, country = match.groups() if match else (None, None)\n",
    "    with open(os.path.join(dirname, 'args.txt')) as f:\n",
    "        lambda_ = f.readlines()[13].split()[1:]\n",
    "        assert all([l == lambda_[0] for l in lambda_])\n",
    "        lambda_ = lambda_[0]\n",
    "    with open(os.path.join(dirname, 'log.txt')) as f:\n",
    "        lines = f.readlines()\n",
    "        epochline = [l for l in lines if \"最小 val loss (epoch\" in l]\n",
    "        if len(epochline) < 1: continue\n",
    "        epoch = re.search(r\"epoch (.*)\\)\", epochline[0]).groups()[0]\n",
    "        maeline = [l for l in lines if \"[err(val/test)]\" in l]\n",
    "        if len(maeline) < 2: continue\n",
    "        mae = re.search(r\"\\[err\\(val/test\\)\\] .*?/(.*?),\", maeline[-1]).groups()[0]\n",
    "    res.update({(lambda_, shift, country): (mae, epoch)})\n",
    "# print(sorted(res.keys(), key=lambda x: (x[0], int(x[1]), {\"England\": 0, \"France\": 1, \"Italy\": 2, \"Spain\": 3}.get(x[2], 4))))\n",
    "\n",
    "_s = []\n",
    "for l in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    __s = []\n",
    "    for s in [2, 6, 13]:\n",
    "        for c in [\"England\", \"France\", \"Italy\", \"Spain\"]:\n",
    "            l, s, c = map(str, [l, s, c])\n",
    "            if (l, s, c) in res: __s.append(res[(l, s, c)])\n",
    "            else: __s.append(('-', '-'))\n",
    "    _s.append(__s)\n",
    "print('\\n'.join(['\\t'.join(['\\t'.join(___s) for ___s in __s]) for __s in _s]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python main.py --country England --shift 6  --model mpnn_lstm --result-dir tests_1119 --exp 6 --device 3\n",
    "# 三个尺度 四个国家 连个模型\n",
    "res = ''\n",
    "i = 0\n",
    "devices = (7, 8, 9)\n",
    "# for device, model in zip((7, 8, 9), ('mpnn_lstm', 'lstm', 'dynst')):\n",
    "# for model in ('dynst',):\n",
    "for shift in [2, 6, 13]:\n",
    "    for country in [f'SIM{i}' for i in range(5)]:\n",
    "        res += f'python main.py --country {country:7s} --shift {shift:2} --dataset sim --result-dir tests_1209 --exp 2_sim_graph_lambda_0 --device {devices[i % len(devices)]}\\n'\n",
    "        i += 1\n",
    "# print('\\n'.join(sorted(res.split('\\n')[:-1], key=lambda x: int(x[-1]))))\n",
    "print(semi(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math\n",
    "\n",
    "dataset = 'flunet'\n",
    "\n",
    "node_observed_ratios = [50, 80]\n",
    "# countries = [\"England\", \"France\", \"Spain\", \"Italy\"]\n",
    "# countries = [\"Japan\"]\n",
    "countries = [\"h1n1\", \"h3n2\", \"BV\", \"BY\"]\n",
    "shifts = [2, 6, 13]\n",
    "\n",
    "# 单独国家和尺度 9个 graph-lambda\n",
    "def gen_lambda(mode, devices, result_dir, commoncmd = \"\"):\n",
    "    assert mode in ['baseline', 'dynst', 'baselinedynst']\n",
    "    keys = tuple((country, shift) for country in countries for shift in shifts)\n",
    "    res = ''\n",
    "    i = 0\n",
    "    for country, shift in keys:\n",
    "        assert country in countries and shift in shifts\n",
    "        for node_observed_ratio in node_observed_ratios:\n",
    "            if 'baseline' in mode:\n",
    "                # baselines\n",
    "                res += f'python main.py --dataset {dataset} {commoncmd} --country {country} --shift {shift:2} --result-dir {result_dir} --exp 1_baselines_mpnn_lstm_{node_observed_ratio} --node-observed-ratio {node_observed_ratio} --model mpnn_lstm        --device {devices[i % len(devices)]}\\n'\n",
    "                i += 1\n",
    "                res += f'python main.py --dataset {dataset} {commoncmd} --country {country} --shift {shift:2} --result-dir {result_dir} --exp 1_baselines_mpnn_tl_{node_observed_ratio}   --node-observed-ratio {node_observed_ratio} --model mpnn_lstm --maml --device {devices[i % len(devices)]}\\n'\n",
    "                i += 1\n",
    "                res += f'python main.py --dataset {dataset} {commoncmd} --country {country} --shift {shift:2} --result-dir {result_dir} --exp 1_baselines_lstm_{node_observed_ratio}      --node-observed-ratio {node_observed_ratio} --model lstm             --device {devices[i % len(devices)]}\\n'\n",
    "                i += 1\n",
    "            if 'dynst' in mode:\n",
    "                # 按 graph-lambda 跑实验\n",
    "                for num_graph_lambda in range(10):\n",
    "                    for i_hp in range(4):\n",
    "                        exp_name = f\"graph_lambda_{int(num_graph_lambda)}\" + ((\"_no_graph\" if i_hp & 1 else \"\") + (\"_no_virtual_node\" if i_hp & 2 else \"\") if i_hp else \"\")\n",
    "                        exp_hyperparams = (\" --no-graph\" if i_hp & 1 else \"\") + (\" --no-virtual-node\" if i_hp & 2 else \"\")\n",
    "                        res += f'python main.py --dataset {dataset} {commoncmd} --country {country} --shift {shift:2} --result-dir {result_dir} --exp 2_{node_observed_ratio}_{exp_name:39} --node-observed-ratio {node_observed_ratio} --graph-lambda {num_graph_lambda / 10} {exp_hyperparams:29} --device {devices[i % len(devices)]}\\n'\n",
    "                        i += 1\n",
    "            if not ('baseline' in mode or 'dynst' in mode):\n",
    "                raise NotImplementedError(f\"这里不应该被执行 {mode} {'baseline' in mode}\")\n",
    "\n",
    "    sorted_res = sorted([x for x in res.split('\\n') if not x == ''], key=lambda x: int(re.search(\"device +(.*)\", x).groups()[0]))\n",
    "    return '\\n'.join(sorted_res)\n",
    "\n",
    "def semi(res, num_works_per_device=2):\n",
    "    # 同一个device串行执行几个程序\n",
    "\n",
    "    grouped_commands = {}\n",
    "    for command in res.split(\"\\n\"):\n",
    "        if command == '': continue\n",
    "        device = re.search(r\"device (.*)\", command).groups()[0]\n",
    "        \n",
    "        if device in grouped_commands:\n",
    "            grouped_commands[device].append(command)\n",
    "        else:\n",
    "            grouped_commands[device] = [command]\n",
    "\n",
    "    # 2. 对每个分组进行处理\n",
    "    semicolon_joined_commands = []\n",
    "    for device, device_commands in grouped_commands.items():\n",
    "        works = math.ceil(len(device_commands) / num_works_per_device)\n",
    "        for i in range(0, len(device_commands), works):\n",
    "            semicolon_joined_commands.append(\"; \".join(device_commands[i:i+works]))\n",
    "\n",
    "    return '\\n' * 2 + '\\n'.join(semicolon_joined_commands) + '\\n' * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 跑实验\n",
    "result_dir = \"test_0106_all\"\n",
    "devices = [3, 7, 8, 9]\n",
    "i = 0\n",
    "res = \"\"\n",
    "for node_observed_ratio in [50, 80]:\n",
    "    for shift in [2, 6, 13]:\n",
    "        for country in [\"England\", \"France\", \"Italy\", \"Spain\"]:\n",
    "            res += f\"python main.py --dataset dataforgood --country {country} --shift {shift:2} --result-dir {result_dir} --exp 2_{node_observed_ratio}_all --node-observed-ratio {node_observed_ratio} --device {devices[i % len(devices)]}\\n\"\n",
    "            i += 1\n",
    "        res += f\"python main.py --dataset japan --country Japan --shift {shift:2} --result-dir {result_dir} --exp 2_{node_observed_ratio}_all --node-observed-ratio {node_observed_ratio} --device {devices[i % len(devices)]}\\n\"\n",
    "        i += 1\n",
    "\n",
    "print(semi(res, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "date_result_dir = '0209'\n",
    "# res = gen_lambda((\"Italy\", 2))\n",
    "# res = gen_lambda((\"Italy\", 2), (\"Spain\", 2), (\"England\", 6), (\"France\", 6), (\"England\", 13), (\"Spain\", 13))\n",
    "# res = gen_lambda('dynst', [2, 3, 4, 7, 8, 9], result_dir)\n",
    "# res = gen_lambda('baseline', [9], result_dir, \"--seed 5 --gendata\")\n",
    "res = '\\n'.join([gen_lambda('baselinedynst', [2, 4, 5, 6, 7, 8, 9], f\"tests_{date_result_dir}_seed{i}\", f\"--seed-dataset {i}\") for i in range(6, 11)])\n",
    "# print(res)\n",
    "print(semi(res, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from show_result import show_result\n",
    "\n",
    "textdir, exp = \"tests_0205_seed1\", 2\n",
    "baseline_orders = ['lstm', 'mpnn_lstm', 'mpnn_tl']\n",
    "ablation_orders = ['no_graph', 'no_virtual_node', 'no_graph_no_virtual_node']\n",
    "\n",
    "def sort_key(e):\n",
    "    # return 1\n",
    "    match = re.search(r\"exp_\\d+_(\\d+)_graph_lambda_(\\d+)_?([a-z_]+)?\", e)\n",
    "    if match:\n",
    "        _node_observed_ratio, _lambda, _ablation = match.groups()\n",
    "        return 1, int(_node_observed_ratio), int(_lambda), {e: i for i, e in enumerate(ablation_orders)}.get(_ablation, -1)\n",
    "    else:\n",
    "        _baseline, _node_observed_ratio = re.search(r\"baselines_(.*)_(\\d+)\", e).groups()\n",
    "        return 0, int(_node_observed_ratio), baseline_orders.index(_baseline)\n",
    "\n",
    "# show_result(f\"results/tests_{textdir}/exp_{exp}\", mode=0)\n",
    "res = []\n",
    "for d in sorted(os.listdir(f\"results/{textdir}\"), key=sort_key):\n",
    "    if not f'exp_{exp}' in d: continue\n",
    "    # res.append(d)\n",
    "    res.append(show_result(os.path.join(f\"results/{textdir}\", d), mode=1))\n",
    "\n",
    "res1 = '\\n'.join(res)\n",
    "res2 = re.sub(r'\\n+', '\\n', res1.replace('''flunet\n",
    "[err_test] h1n1,h3n2,BV,BY\n",
    "\n",
    "7->1 (w7s2) | 7->1 (w7s6) | 7->1 (w7s13)''', ''))\n",
    "# res_str = '\\n'.join([_r for r in res for _r in r.split('\\n') if len(_r) > 1 and (_r[0] == '-' or _r[0].isdigit() and _r[1] != '-')])\n",
    "# print(res_str)\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_process.dataforgood import load_data\n",
    "from utils.args import get_parser, process_args\n",
    "args = get_parser().parse_args()\n",
    "args.country = 'NewZealand'\n",
    "args = process_args(args, False)\n",
    "meta_data = load_data(args.dataset_cache_dir, args.data_dir, args.dataset, args.batch_size, args.xdays, args.ydays, args.window, args.shift, args.train_ratio, args.val_ratio, 1, enable_cache = True)\n",
    "for c in meta_data[\"dates\"]:\n",
    "    assert meta_data[\"dates\"][c] == sorted(meta_data[\"dates\"][c])\n",
    "    print(f\"{c:10}\", f\"{len(meta_data['regions'][c]):3}\", meta_data[\"dates\"][c][0], meta_data[\"dates\"][c][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(r\"exp_\\d+_\\d+_graph_lambda_(\\d+)_?([a-z_]+)?\", \"exp_2_80_graph_lambda_3\").groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res27 = [r.split('\\t') for r in res_str0.split('\\n')]\n",
    "res28 = [r.split('\\t') for r in res_str.split('\\n')]\n",
    "res = [[''] * 24 for _ in range(30)]  # 创建30个独立的列表\n",
    "\n",
    "for i in range(30):\n",
    "    for j, (r27, r28) in enumerate(zip(res27[i], res28[i])):\n",
    "        if r27 == r28: res[i][j] = r27\n",
    "        else:\n",
    "            if r27 == '-':\n",
    "                res[i][j] = r28\n",
    "            elif r28 == '-':\n",
    "                res[i][j] = r27\n",
    "            else:\n",
    "                print(i, j, r27, r28)\n",
    "                res[i][j] = r28\n",
    "\n",
    "_s = '\\n'.join(['\\t'.join(r) for r in res])\n",
    "print(_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('''-\t-\t-\t-\t9.738\t4\t17.935\t6\t5.205\t220\t1.938\t76\t-\t-\t-\t-\t5.151\t5\t-\t-\t-\t-\t32.895\t1\n",
    "-\t-\t-\t-\t10.572\t342\t18.235\t6\t5.091\t402\t1.492\t50\t-\t-\t-\t-\t5.315\t5\t-\t-\t-\t-\t33.003\t1\n",
    "-\t-\t-\t-\t9.457\t4\t17.918\t6\t5.743\t159\t1.795\t65\t-\t-\t-\t-\t5.043\t5\t-\t-\t-\t-\t32.972\t1\n",
    "-\t-\t-\t-\t9.738\t4\t17.935\t6\t5.205\t220\t1.938\t76\t-\t-\t-\t-\t5.151\t5\t-\t-\t-\t-\t32.895\t1\n",
    "-\t-\t-\t-\t9.44\t347\t18.264\t6\t4.843\t350\t1.597\t47\t-\t-\t-\t-\t5.815\t492\t-\t-\t-\t-\t33.028\t1\n",
    "-\t-\t-\t-\t9.457\t4\t17.918\t6\t5.743\t159\t1.795\t65\t-\t-\t-\t-\t5.043\t5\t-\t-\t-\t-\t32.972\t1\n",
    "-\t-\t-\t-\t9.738\t4\t17.935\t6\t5.205\t220\t1.938\t76\t-\t-\t-\t-\t5.151\t5\t-\t-\t-\t-\t32.895\t1\n",
    "-\t-\t-\t-\t9.119\t389\t18.275\t6\t4.826\t431\t1.999\t163\t-\t-\t-\t-\t5.284\t5\t-\t-\t-\t-\t33.006\t1\n",
    "-\t-\t-\t-\t9.457\t4\t17.918\t6\t5.743\t159\t1.795\t65\t-\t-\t-\t-\t5.043\t5\t-\t-\t-\t-\t32.972\t1\n",
    "-\t-\t-\t-\t9.738\t4\t17.935\t6\t5.205\t220\t1.938\t76\t-\t-\t-\t-\t5.151\t5\t-\t-\t-\t-\t32.895\t1\n",
    "-\t-\t-\t-\t8.286\t477\t18.225\t6\t5.078\t310\t1.485\t47\t-\t-\t-\t-\t5.296\t5\t-\t-\t-\t-\t33.022\t1\n",
    "-\t-\t-\t-\t9.457\t4\t17.918\t6\t5.743\t159\t1.795\t65\t-\t-\t-\t-\t5.043\t5\t-\t-\t-\t-\t32.972\t1\n",
    "-\t-\t-\t-\t9.738\t4\t17.935\t6\t5.205\t220\t1.938\t76\t-\t-\t-\t-\t5.151\t5\t-\t-\t-\t-\t32.895\t1\n",
    "-\t-\t-\t-\t9.661\t412\t26.12\t196\t5.02\t409\t1.556\t44\t-\t-\t-\t-\t5.332\t5\t-\t-\t-\t-\t33.019\t1\n",
    "-\t-\t-\t-\t9.457\t4\t17.918\t6\t5.743\t159\t1.795\t65\t-\t-\t-\t-\t5.043\t5\t-\t-\t-\t-\t32.972\t1\n",
    "-\t-\t-\t-\t9.738\t4\t17.935\t6\t5.205\t220\t1.938\t76\t-\t-\t-\t-\t5.151\t5\t-\t-\t-\t-\t32.895\t1\n",
    "-\t-\t-\t-\t9.624\t341\t18.246\t6\t5.124\t454\t1.584\t50\t-\t-\t-\t-\t5.587\t464\t-\t-\t-\t-\t33.01\t1\n",
    "-\t-\t-\t-\t9.457\t4\t17.918\t6\t5.743\t159\t1.795\t65\t-\t-\t-\t-\t5.043\t5\t-\t-\t-\t-\t32.972\t1\n",
    "-\t-\t-\t-\t9.738\t4\t17.935\t6\t5.205\t220\t1.938\t76\t-\t-\t-\t-\t5.151\t5\t-\t-\t-\t-\t32.895\t1\n",
    "-\t-\t-\t-\t9.591\t412\t18.115\t6\t4.904\t498\t1.509\t54\t-\t-\t-\t-\t5.353\t5\t-\t-\t-\t-\t33.001\t1\n",
    "-\t-\t-\t-\t9.457\t4\t17.918\t6\t5.743\t159\t1.795\t65\t-\t-\t-\t-\t5.043\t5\t-\t-\t-\t-\t32.972\t1\n",
    "-\t-\t-\t-\t9.738\t4\t17.935\t6\t5.205\t220\t1.938\t76\t-\t-\t-\t-\t5.151\t5\t-\t-\t-\t-\t32.895\t1\n",
    "-\t-\t-\t-\t11.126\t477\t31.764\t301\t4.883\t341\t2.092\t444\t-\t-\t-\t-\t5.309\t5\t-\t-\t-\t-\t33.001\t1\n",
    "-\t-\t-\t-\t9.457\t4\t17.918\t6\t5.743\t159\t1.795\t65\t-\t-\t-\t-\t5.043\t5\t-\t-\t-\t-\t32.972\t1\n",
    "-\t-\t-\t-\t9.738\t4\t17.935\t6\t5.205\t220\t1.938\t76\t-\t-\t-\t-\t5.151\t5\t-\t-\t-\t-\t32.895\t1\n",
    "-\t-\t-\t-\t9.205\t295\t18.193\t6\t5.133\t491\t1.517\t54\t-\t-\t-\t-\t5.256\t5\t-\t-\t-\t-\t33.026\t1\n",
    "-\t-\t-\t-\t9.457\t4\t17.918\t6\t5.743\t159\t1.795\t65\t-\t-\t-\t-\t5.043\t5\t-\t-\t-\t-\t32.972\t1''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise OSError(\"这是检查 mask 的地区是否一致的函数\")\n",
    "import pickle as p, os\n",
    "meta_datas = []\n",
    "for f in os.listdir(\"dataset_cache\"):\n",
    "    # 假设 f 是文件路径\n",
    "    with open(os.path.join(\"dataset_cache\", f), \"rb\") as file:\n",
    "        meta_datas.append(p.load(file))\n",
    "\n",
    "# [[list(meta_datas[i][\"selected_indices\"][name]) for i in range(len(meta_datas))] for name in meta_datas[0][\"country_names\"]]\n",
    "for name in meta_datas[0][\"country_names\"]:\n",
    "    _l = [list(meta_datas[i][\"selected_indices\"][\"England\"]) for i in range(len(meta_datas))]\n",
    "    print(name, all([_l[i] == _l[0] for i in range(1, len(_l))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError('执行前慎重')\n",
    "import os, torch\n",
    "for dirpath, dirnames, files in os.walk(\"results/tests_1015\"):\n",
    "    if not len(files): continue\n",
    "    for file in files:\n",
    "        if file.endswith(\".pth\"):\n",
    "            _model = torch.load(os.path.join(dirpath, file))\n",
    "            _state_dict = _model.state_dict()\n",
    "            os.rename(os.path.join(dirpath, file), os.path.join(dirpath, file + '.bak'))\n",
    "            torch.save(_state_dict, os.path.join(dirpath, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from lxml import html\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # 网址模板\n",
    "# url_template = \"https://findthatpostcode.uk/areas/{}.html\"\n",
    "\n",
    "# # 你要查询的行政区划代码列表\n",
    "codes = ['E06000001', 'E06000002', 'E06000003', 'E06000004', 'E06000005', 'E06000006', 'E06000007', 'E06000008', 'E06000009', 'E06000011', 'E06000012', 'E06000013', 'E06000014', 'E06000015', 'E06000016', 'E06000017', 'E06000018', 'E06000020', 'E06000021', 'E06000022', 'E06000024', 'E06000026', 'E06000027', 'E06000030', 'E06000031', 'E06000032', 'E06000033', 'E06000034', 'E06000035', 'E06000036', 'E06000037', 'E06000038', 'E06000039', 'E06000040', 'E06000041', 'E06000042', 'E06000043', 'E06000044', 'E06000045', 'E06000046', 'E06000047', 'E06000049', 'E06000050', 'E06000051', 'E06000052', 'E06000054', 'E06000055', 'E06000056', 'E06000057', 'E06000059', 'E08000001', 'E08000002', 'E08000003', 'E08000004', 'E08000005', 'E08000007', 'E08000008', 'E08000009', 'E08000010', 'E08000011', 'E08000012', 'E08000013', 'E08000014', 'E08000015', 'E08000016', 'E08000017', 'E08000018', 'E08000019', 'E08000021', 'E08000022', 'E08000023', 'E08000024', 'E08000025', 'E08000026', 'E08000027', 'E08000030', 'E08000031', 'E08000032', 'E08000033', 'E08000034', 'E08000035', 'E08000036', 'E08000037', 'E09000001', 'E09000002', 'E09000003', 'E09000004', 'E09000005', 'E09000006', 'E09000008', 'E09000009', 'E09000010', 'E09000015', 'E09000016', 'E09000017', 'E09000018', 'E09000020', 'E09000021', 'E09000022', 'E09000023', 'E09000026', 'E09000027', 'E09000029', 'E10000002', 'E10000003', 'E10000006', 'E10000007', 'E10000008', 'E10000011', 'E10000012', 'E10000013', 'E10000014', 'E10000015', 'E10000016', 'E10000017', 'E10000018', 'E10000019', 'E10000020', 'E10000021', 'E10000023', 'E10000024', 'E10000025', 'E10000027', 'E10000028', 'E10000029', 'E10000030', 'E10000031', 'E10000032', 'E10000034']\n",
    "# # 存储结果的列表\n",
    "# results = []\n",
    "\n",
    "# # 遍历每个代码，访问对应的网址\n",
    "# for code in tqdm(codes):\n",
    "#     url = url_template.format(code)\n",
    "#     response = requests.get(url)\n",
    "#     tree = html.fromstring(response.content)\n",
    "#     h2_text = tree.xpath('//html/body/main/header/h2/text()')\n",
    "#     results.append(h2_text)\n",
    "\n",
    "# # 打印结果\n",
    "# for result in results:\n",
    "#     print(result)\n",
    "# str([i[0][1:-2].strip() for i in results])\n",
    "names = ['Hartlepool', 'Middlesbrough', 'Redcar and Cleveland', 'Stockton-on-Tees', 'Darlington', 'Halton', 'Warrington', 'Blackburn with Darwen', 'Blackpool', 'East Riding of Yorkshire', 'North East Lincolnshire', 'North Lincolnshire', 'York', 'Derby', 'Leicester', 'Rutland', 'Nottingham', 'Telford and Wrekin', 'Stoke-on-Trent', 'Bath and North East Somerset', 'North Somerset', 'Plymouth', 'Torbay', 'Swindon', 'Peterborough', 'Luton', 'Southend-on-Sea', 'Thurrock', 'Medway', 'Bracknell Forest', 'West Berkshire', 'Reading', 'Slough', 'Windsor and Maidenhead', 'Wokingham', 'Milton Keynes', 'Brighton and Hove', 'Portsmouth', 'Southampton', 'Isle of Wight', 'County Durham', 'Cheshire East', 'Cheshire West and Chester', 'Shropshire', 'Cornwall', 'Wiltshire', 'Bedford', 'Central Bedfordshire', 'Northumberland', 'Dorset', 'Bolton', 'Bury', 'Manchester', 'Oldham', 'Rochdale', 'Stockport', 'Tameside', 'Trafford', 'Wigan', 'Knowsley', 'Liverpool', 'St. Helens', 'Sefton', 'Wirral', 'Barnsley', 'Doncaster', 'Rotherham', 'Sheffield', 'Newcastle upon Tyne', 'North Tyneside', 'South Tyneside', 'Sunderland', 'Birmingham', 'Coventry', 'Dudley', 'Walsall', 'Wolverhampton', 'Bradford', 'Calderdale', 'Kirklees', 'Leeds', 'Wakefield', 'Gateshead', 'City of London', 'Barking and Dagenham', 'Barnet', 'Bexley', 'Brent', 'Bromley', 'Croydon', 'Ealing', 'Enfield', 'Harrow', 'Havering', 'Hillingdon', 'Hounslow', 'Kensington and Chelsea', 'Kingston upon Thames', 'Lambeth', 'Lewisham', 'Redbridge', 'Richmond upon Thames', 'Sutton', 'Buckinghamshire', 'Cambridgeshire', 'Cumbria', 'Derbyshire', 'Devon', 'East Sussex', 'Essex', 'Gloucestershire', 'Hampshire', 'Hertfordshire', 'Kent', 'Lancashire', 'Leicestershire', 'Lincolnshire', 'Norfolk', 'Northamptonshire', 'North Yorkshire', 'Nottinghamshire', 'Oxfordshire', 'Somerset', 'Staffordshire', 'Suffolk', 'Surrey', 'Warwickshire', 'West Sussex', 'Worcestershire']\n",
    "{codes[i]: names[i] for i in range(len(names))}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.utils import progress_indicator\n",
    "\n",
    "\n",
    "# 读取地理数据\n",
    "shapes_zip_path = \"data/mapfiles/gadm41_GBR_shp.zip\"\n",
    "\n",
    "with zipfile.ZipFile(shapes_zip_path) as z:\n",
    "    names = z.namelist()\n",
    "shps = [n for n in names if n.endswith(\"shp\")]\n",
    "\n",
    "qbar = progress_indicator(shps, leave=False)\n",
    "shapes = []\n",
    "\n",
    "for i in range(len(shps)):\n",
    "    qbar.set_description(f\"正在读取 {shps[i]} ({shapes_zip_path})\")\n",
    "    shapes.append(gpd.read_file(f\"zip://{shapes_zip_path}!/{shps[i]}\"))\n",
    "    qbar.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# 指定你的设备\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def convert_model_files(root_dir):\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.pth.bak'):\n",
    "                pth_file = os.path.join(dirpath, filename[:-4])\n",
    "                bak_file = os.path.join(dirpath, filename)\n",
    "                os.rename(bak_file, pth_file)\n",
    "\n",
    "# 替换为你的目标目录\n",
    "target_directory = \"results\"\n",
    "convert_model_files(target_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# command generator\n",
    "\n",
    "def baseline(ratio=70):\n",
    "    date = f\"1023_2_observed_{ratio}\"\n",
    "    countries = [\"England,Spain\", \"France,Italy\"]\n",
    "    shifts = [2, 6, 13]\n",
    "    models = [\"mpnn_lstm\", \"lstm\", \"dynst\"]\n",
    "    cmd_pattern = lambda c, s, m: f\"python main.py --country {c:{max(map(len, countries))}} --result-dir tests_{date} --shift {s:<{max(map(len, map(str, shifts)))}} --exp 0_EN_ES_{m:{max(map(len, models))}} --model {m:{max(map(len, models))}}  --node-observed-ratio {ratio} --device 0\"\n",
    "    \n",
    "    cmds = []\n",
    "    for i_c in range(len(countries)):\n",
    "        for i_s in range(len(shifts)):\n",
    "            for i_m in range(len(models)):\n",
    "                cmds.append(cmd_pattern(countries[i_c], shifts[i_s], models[i_m]))\n",
    "    cmds = '\\n'.join(cmds)\n",
    "    return cmds\n",
    "\n",
    "def cmd(exp_num, ratio = 70):\n",
    "    assert type(exp_num) == int\n",
    "    date = f\"1023_2_observed_70\"\n",
    "    str = f'''\n",
    "python main.py --country France        --result-dir tests_{date} --shift 2  --exp {exp_num} --node-observed-ratio {ratio} --graph-lambda 0.{int(exp_num)} --device 8\n",
    "python main.py --country England       --result-dir tests_{date} --shift 6  --exp {exp_num} --node-observed-ratio {ratio} --graph-lambda 0.{int(exp_num)} --device 8\n",
    "python main.py --country France,Italy  --result-dir tests_{date} --shift 13 --exp {exp_num} --node-observed-ratio {ratio} --graph-lambda 0.{int(exp_num)} --device 9\n",
    "'''\n",
    "    return str\n",
    "print(baseline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.custom_datetime import datetime\n",
    "date = \"1023_2_observed_70\"\n",
    "print(f\"统计 {date} 于 {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "from show_result import extract_results, show_result\n",
    "import os\n",
    "res = []\n",
    "for f in os.listdir(f\"results/tests_{date}\"):\n",
    "    print(\"\\033[32m> {}\\033[0m\".format(f))\n",
    "    dir = f\"results/tests_{date}/{f}\"\n",
    "    try:\n",
    "        res += extract_results(dir)\n",
    "        show_result(dir)\n",
    "    except:\n",
    "        for f in os.listdir(dir):\n",
    "            print(\"\\033[32m> {}\\033[0m\".format(f))\n",
    "            _dir = f\"{dir}/{f}\"\n",
    "            res += extract_results(_dir)\n",
    "            show_result(_dir)\n",
    "            print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res0 = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError\n",
    "from utils.args import parse_args\n",
    "from utils.data_process.dataforgood import load_data\n",
    "from argparse import Namespace\n",
    "preprocessed_data_dir, data_dir, databinfile, batch_size, xdays, ydays, window, shift, train_ratio, val_ratio, node_observed_ratio = ('data_preprocessed', 'data/dataforgood', 'data_preprocessed/dataforgood_x7_y1_w7_s2_m50.bin', 8, 7, 1, 7, 2, 0.7, 0.1, 1)\n",
    "args = Namespace(preprocessed_data_dir=preprocessed_data_dir, data_dir=data_dir, databinfile=databinfile, batch_size=batch_size, xdays=xdays, ydays=ydays, window=window, shift=shift, train_ratio=train_ratio, val_ratio=val_ratio, node_observed_ratio=node_observed_ratio)\n",
    "res = load_data(args, False)\n",
    "\n",
    "adjs = [v[1][2] for v in res[\"data\"].values()]\n",
    "nonzeros = [a.count_nonzero((1,2)) for a in adjs]\n",
    "\n",
    "nodes = [i.shape[1] for i in adjs]\n",
    "\n",
    "[nonzeros[i] / (nodes[i] * nodes[i]) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T05:22:25.802814Z",
     "start_time": "2024-09-23T05:22:25.797529Z"
    }
   },
   "outputs": [],
   "source": [
    "raise NotImplementedError\n",
    "import os\n",
    "\n",
    "def delete_pth_files(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.pth'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted: {file_path}\")\n",
    "\n",
    "# 使用示例\n",
    "delete_pth_files('results/tests_old')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
