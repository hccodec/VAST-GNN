[2024-09-20 17:00:59,100      exp_logger INFO] 使用 GPU 7 进行计算
[2024-09-20 17:00:59,100      exp_logger INFO] 运行结果将保存至 results/results_test/tmp/dataforgood/21_7_w21_dynst_graphlearner_20240920170059
[2024-09-20 17:00:59,549      exp_logger INFO] 已从数据文件读取 dataforgood 数据集
[2024-09-20 17:00:59,549      exp_logger INFO] 开始训练 Italy
[2024-09-20 17:01:04,117      exp_logger INFO] # 129668 params
[2024-09-20 17:01:06,683      exp_logger INFO] 出现错误，中断训练
[2024-09-20 17:01:06,683      exp_logger INFO] CUDA out of memory. Tried to allocate 4.99 GiB. GPU 7 has a total capacty of 23.69 GiB of which 4.90 GiB is free. Including non-PyTorch memory, this process has 9.40 GiB memory in use. Process 2563029 has 9.39 GiB memory in use. Of the allocated memory 5.30 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2024-09-20 17:01:06,684      exp_logger INFO] Traceback (most recent call last):
[2024-09-20 17:01:06,684      exp_logger INFO]   File "/home/hbj/workspace/vscode_workspace/lp/workspace/custom/utils/utils.py", line 145, in wrapper
[2024-09-20 17:01:06,684      exp_logger INFO]     return f(*args, **kwargs)
[2024-09-20 17:01:06,684      exp_logger INFO]   File "/home/hbj/workspace/vscode_workspace/lp/workspace/custom/train_test.py", line 113, in train_process
[2024-09-20 17:01:06,684      exp_logger INFO]     loss.backward(retain_graph=True)
[2024-09-20 17:01:06,684      exp_logger INFO]   File "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
[2024-09-20 17:01:06,684      exp_logger INFO]     torch.autograd.backward(
[2024-09-20 17:01:06,684      exp_logger INFO]   File "/home/hbj/anaconda3/envs/lp/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
[2024-09-20 17:01:06,684      exp_logger INFO]     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[2024-09-20 17:01:06,684      exp_logger INFO] torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.99 GiB. GPU 7 has a total capacty of 23.69 GiB of which 4.90 GiB is free. Including non-PyTorch memory, this process has 9.40 GiB memory in use. Process 2563029 has 9.39 GiB memory in use. Of the allocated memory 5.30 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2024-09-20 17:01:06,684      exp_logger INFO] 
[2024-09-20 17:01:06,686      exp_logger INFO] 实验结果已保存至 results/results_test/tmp/dataforgood/21_7_w21_dynst_graphlearner_20240920170059
